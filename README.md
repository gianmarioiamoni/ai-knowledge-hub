# AI Knowledge Hub

Enterprise-grade multi-tenant RAG (Retrieval-Augmented Generation) platform built with Next.js 15, Supabase, LangChain, and OpenAI. Transform your company's documentation into an AI-powered knowledge assistant that answers questions, generates SOPs, and scales with your team.

## ğŸ¯ What It Does

- **Document Ingestion**: Upload PDFs, automatically extract text, chunk intelligently, and vectorize with OpenAI embeddings
- **RAG Chat**: Ask questions in natural language, get contextual answers with source attribution
- **SOP Generation**: Auto-generate Standard Operating Procedures from your documentation
- **Team Management**: Role-based access control (RBAC) with 4 permission levels
- **Multi-tenancy**: Complete data isolation per organization with Row-Level Security
- **Subscriptions**: Stripe-powered billing with trial, SMB, and Enterprise tiers
- **Bilingual**: Full internationalization (English/Italian) with `next-intl`

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Next.js 15 App                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  App Router    â”‚  â”‚ Server Actions â”‚  â”‚  API Routes    â”‚    â”‚
â”‚  â”‚  (RSC-first)   â”‚  â”‚  (mutations)   â”‚  â”‚  (streaming)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Supabase Backend                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Auth + RLS   â”‚  â”‚ PostgreSQL + â”‚  â”‚ Storage (PDFs)  â”‚       â”‚
â”‚  â”‚ (multi-tenant)â”‚  â”‚   pgvector   â”‚  â”‚                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AI Layer                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  LangChain   â”‚  â”‚ OpenAI API   â”‚  â”‚  Vector Search  â”‚       â”‚
â”‚  â”‚ (orchestration)â”‚ â”‚(GPT-4.1/embed)â”‚ â”‚ (cosine sim.)   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¤– AI Implementation Deep Dive

### Document Ingestion Pipeline

The ingestion pipeline transforms unstructured PDFs into searchable vector embeddings:

```typescript
// lib/server/langchain.ts
1. Text Splitting (RecursiveCharacterTextSplitter)
   - Chunk size: 900 characters
   - Overlap: 150 characters (maintains context continuity)
   - Recursive splitting by: paragraphs â†’ sentences â†’ characters
   
2. Embedding Generation (OpenAI text-embedding-3-small)
   - Dimensions: 1536 (standard OpenAI embedding size)
   - Model: text-embedding-3-small (optimized cost/performance)
   - Batch processing for efficiency
   
3. Vector Storage (Supabase pgvector)
   - Column type: vector(1536)
   - Index: IVFFlat for fast approximate nearest neighbor search
   - RLS policies enforce organization-level isolation
```

**Why these parameters?**
- **900 char chunks**: Balance between context richness and precision
- **150 char overlap**: Prevents loss of information at chunk boundaries
- **1536 dimensions**: OpenAI's standard, proven effective for semantic search

### RAG (Retrieval-Augmented Generation) Flow

```typescript
// app/api/chat/query/route.ts

Step 1: Query Embedding
â”œâ”€ Convert user question to 1536-dim vector
â”œâ”€ Use same embedding model as ingestion (consistency)
â””â”€ ~50-100ms latency

Step 2: Vector Similarity Search (lib/server/rag.ts)
â”œâ”€ SQL function: match_document_chunks()
â”œâ”€ Cosine similarity: 1 - (embedding <=> query_embedding)
â”œâ”€ Filter: organization_id (RLS enforcement)
â”œâ”€ Threshold: 0.75 (configurable, balances precision/recall)
â”œâ”€ Top-K: 6 chunks retrieved
â””â”€ ~100-200ms latency

Step 3: Context Construction
â”œâ”€ Format: "(1) chunk_text\n\n(2) chunk_text..."
â”œâ”€ Numbered for easy reference
â””â”€ Concatenate top 6 chunks

Step 4: Prompt Engineering (lib/server/ragPrompt.ts)
â”œâ”€ System: "You are a concise assistant..."
â”œâ”€ Instruction: "Answer based only on provided context"
â”œâ”€ Fallback: "Say you don't have enough information"
â”œâ”€ Context injection: Retrieved chunks
â””â”€ User question

Step 5: LLM Generation (GPT-4.1-mini)
â”œâ”€ Temperature: 0.2 (focused, deterministic)
â”œâ”€ Streaming: Token-by-token response
â”œâ”€ Model: gpt-4.1-mini (cost-effective for RAG)
â””â”€ ~1-3s latency for complete answer

Step 6: Response Streaming (NDJSON format)
â”œâ”€ Line 1: { type: "meta", conversationId, chunks[] }
â”œâ”€ Lines 2-N: { type: "token", data: "word" }
â”œâ”€ Last line: { type: "done" }
â””â”€ Client receives incremental UI updates
```

### Vector Search SQL Function

```sql
-- supabase/migrations/0002_match_function.sql
create function match_document_chunks (
  query_embedding vector(1536),
  match_count int,
  org_id uuid,
  similarity_threshold float8 default 0.75
) returns table (...) as $$
  select
    dc.id,
    dc.chunk_text,
    1 - (dc.embedding <=> query_embedding) as similarity
  from document_chunks dc
  where dc.organization_id = org_id
  order by dc.embedding <=> query_embedding
  limit match_count
$$;
```

**Operator `<=>`**: PostgreSQL pgvector cosine distance
- Range: [0, 2] where 0 = identical vectors
- Conversion: similarity = 1 - distance â†’ [0, 1] where 1 = identical
- Indexed for O(log n) search with IVFFlat

### SOP Generation Pipeline

SOPs (Standard Operating Procedures) leverage the same RAG infrastructure but with specialized prompting:

```typescript
1. Retrieve relevant chunks (similar to chat)
2. Specialized prompt template:
   - "Generate a structured SOP with:"
   - Purpose and scope section
   - Prerequisites checklist
   - Numbered step-by-step instructions
   - Safety warnings and notes
   - Post-completion checklist
3. GPT-4.1 (full model, not mini) for complex reasoning
4. Temperature: 0.3 (slightly higher for creativity)
5. Markdown formatting for easy export
```

### AI Models Selection Rationale

| Use Case | Model | Why? |
|----------|-------|------|
| **Embeddings** | text-embedding-3-small | Cost-effective ($0.02/1M tokens), fast, 1536-dim standard |
| **RAG Chat** | gpt-4.1-mini | 10x cheaper than GPT-4, sufficient for factual Q&A |
| **SOP Generation** | gpt-4.1 | Complex reasoning, structured output, worth the cost |
| **Fallback** | gpt-4o-mini | For any future auxiliary tasks |

### Performance Optimizations

1. **Caching**
   - Embedding cache (planned): Store frequently queried embeddings
   - Conversation cache: Reuse context within same conversation

2. **Rate Limiting**
   - Chat: 20 queries/min per user
   - Prevents abuse and controls OpenAI costs
   - Graceful degradation with retry hints

3. **Streaming Responses**
   - NDJSON streaming: Immediate user feedback
   - Progressive rendering: Better perceived performance
   - Lower TTFB (Time To First Byte)

4. **Batch Operations**
   - Chunk embeddings: Process 10-50 at once
   - Database writes: Bulk insert chunks
   - Reduces API calls and latency

## ğŸ” Security & Multi-tenancy

### Row-Level Security (RLS)

Every table has RLS policies that enforce organization-level isolation:

```sql
-- Example: document_chunks table
create policy "Users can only see their org's chunks"
  on document_chunks
  for select
  using (organization_id = (
    select organization_id 
    from organization_members 
    where user_id = auth.uid()
  ));
```

### Role-Based Access Control (RBAC)

Four roles with hierarchical permissions:

```typescript
VIEWER         â†’ Can: Read docs, use chat, view SOPs
â†“
CONTRIBUTOR    â†’ Can: Upload docs, generate SOPs, export
â†“
COMPANY_ADMIN  â†’ Can: Invite users, manage team, billing
â†“
SUPER_ADMIN    â†’ Can: Manage all orgs, platform oversight
```

Enforced at three levels:
1. **UI**: Hide features based on role
2. **API**: Server-side validation in route handlers
3. **Database**: RLS policies + role column checks

## ğŸ› ï¸ Tech Stack

### Frontend
- **Next.js 15**: App Router, React Server Components (RSC-first architecture)
- **TypeScript**: Strict mode, no `any` allowed
- **Tailwind CSS**: Utility-first styling
- **shadcn/ui**: Accessible component primitives (Radix UI)
- **next-intl**: i18n with server-side translation loading

### Backend & Infrastructure
- **Supabase**:
  - Auth: Email/password + OAuth (Google)
  - Database: PostgreSQL 15+ with pgvector extension
  - Storage: Encrypted file storage for PDFs
  - RLS: Row-level security for multi-tenancy
- **LangChain**: RAG orchestration, text splitting, embeddings
- **OpenAI**: GPT-4.1, GPT-4.1-mini, text-embedding-3-small
- **Stripe**: Subscriptions, checkout, webhooks
- **Nodemailer**: Transactional emails (Gmail SMTP / custom SMTP)

### Data Flow
```
User Upload â†’ Supabase Storage â†’ Server Action
           â†“
  Text Extraction (PDF parsing)
           â†“
  Text Splitting (LangChain, 900 char chunks)
           â†“
  Embedding Generation (OpenAI API, 1536-dim)
           â†“
  Vector Storage (PostgreSQL + pgvector)
           â†“
  Available for RAG Chat & SOP Generation
```

## ğŸš€ Getting Started

### Prerequisites
- Node.js 18+ (for Next.js 15)
- Supabase project with pgvector enabled
- OpenAI API key with GPT-4 access
- Stripe account (for billing)

### Installation

```bash
# Clone and install
git clone <repo-url>
cd ai-knowledge-hub
npm install

# Setup environment
cp .env.example .env.local
# Fill in required values (see below)

# Run migrations
npx supabase db push

# Start dev server
npm run dev
```

### Environment Variables

```bash
# Supabase (required)
NEXT_PUBLIC_SUPABASE_URL=https://xxx.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=eyJxxx...
SUPABASE_SERVICE_ROLE_KEY=eyJxxx...  # For admin operations

# OpenAI (required)
OPENAI_API_KEY=sk-xxx...  # Must have GPT-4 access

# Email - Option 1: Gmail (easiest)
GMAIL_USER=your-email@gmail.com
GMAIL_APP_PASSWORD=xxxx xxxx xxxx xxxx  # Generate at accounts.google.com

# Email - Option 2: Custom SMTP
SMTP_HOST=smtp.your-domain.com
SMTP_PORT=587
SMTP_USER=noreply@your-domain.com
SMTP_PASSWORD=xxx
SMTP_FROM=AI Knowledge Hub <noreply@your-domain.com>

# Admin notifications
ADMIN_EMAIL=admin@your-domain.com
SUPERADMIN_EMAIL=superadmin@your-domain.com  # Optional

# Stripe (required for billing)
STRIPE_SECRET_KEY=sk_test_xxx...
STRIPE_WEBHOOK_SECRET=whsec_xxx...
STRIPE_PRICE_SMB_MONTHLY=price_xxx
STRIPE_PRICE_SMB_ANNUAL=price_xxx
STRIPE_PRICE_ENTERPRISE_MONTHLY=price_xxx
STRIPE_PRICE_ENTERPRISE_ANNUAL=price_xxx

# App config
NEXT_PUBLIC_SITE_URL=http://localhost:3000  # Or production URL
CRON_SECRET=random-string-here  # For /api/cron/reminders

# Optional: Error monitoring
SENTRY_DSN=https://xxx@xxx.ingest.sentry.io/xxx
```

### Stripe Setup

1. Create products and prices in Stripe Dashboard
2. Copy price IDs to `.env.local`
3. Setup webhook endpoint:
   ```bash
   # Development
   stripe listen --forward-to localhost:3000/api/stripe/webhook
   
   # Production
   Add webhook URL in Stripe Dashboard:
   https://your-domain.com/api/stripe/webhook
   
   Events to listen for:
   - customer.subscription.created
   - customer.subscription.updated
   - customer.subscription.deleted
   - checkout.session.completed
   ```

### Database Migrations

Migrations are in `supabase/migrations/`:

1. **0001_init.sql**: Core tables, RLS policies, pgvector setup
2. **0002_match_function.sql**: Vector search SQL function
3. **0003_roles_and_invites.sql**: RBAC tables and invitation system

Apply with:
```bash
npx supabase db push
# Or manually in Supabase SQL Editor
```

## ğŸ“ Project Structure

```
ai-knowledge-hub/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ [locale]/              # Internationalized routes
â”‚   â”‚   â”œâ”€â”€ dashboard/         # Main user dashboard
â”‚   â”‚   â”œâ”€â”€ documents/         # Upload & ingestion UI
â”‚   â”‚   â”œâ”€â”€ chat/              # RAG chat interface
â”‚   â”‚   â”œâ”€â”€ procedures/        # SOP management
â”‚   â”‚   â”œâ”€â”€ admin/             # Team management (COMPANY_ADMIN)
â”‚   â”‚   â”œâ”€â”€ admin-stats/       # Platform stats (SUPER_ADMIN)
â”‚   â”‚   â””â”€â”€ profile/           # User settings
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/query/        # RAG streaming endpoint
â”‚   â”‚   â”œâ”€â”€ stripe/webhook/    # Subscription events
â”‚   â”‚   â””â”€â”€ cron/reminders/    # Scheduled jobs
â”‚   â””â”€â”€ actions/               # Server Actions (mutations)
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ admin/                 # Admin UI components
â”‚   â”œâ”€â”€ chat/                  # Chat interface
â”‚   â”œâ”€â”€ dashboard/             # Dashboard widgets
â”‚   â”œâ”€â”€ documents/             # Document management
â”‚   â”œâ”€â”€ procedures/            # SOP generation UI
â”‚   â”œâ”€â”€ navigation/            # TopNav, CommandPalette
â”‚   â””â”€â”€ ui/                    # shadcn/ui components
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ server/                # Server-only utilities
â”‚   â”‚   â”œâ”€â”€ langchain.ts       # LangChain config & factories
â”‚   â”‚   â”œâ”€â”€ rag.ts             # Vector search & embedding
â”‚   â”‚   â”œâ”€â”€ ragPrompt.ts       # Prompt engineering
â”‚   â”‚   â”œâ”€â”€ supabaseUser.ts    # User-scoped Supabase client
â”‚   â”‚   â”œâ”€â”€ supabaseService.ts # Service role client (admin)
â”‚   â”‚   â”œâ”€â”€ roles.ts           # RBAC helpers
â”‚   â”‚   â””â”€â”€ rateLimit.ts       # In-memory rate limiting
â”‚   â”œâ”€â”€ env.ts                 # Environment validation (Zod)
â”‚   â””â”€â”€ seo.ts                 # Metadata generation
â”œâ”€â”€ supabase/
â”‚   â””â”€â”€ migrations/            # SQL migrations
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ USER_MANUAL.md         # English user guide
â”‚   â””â”€â”€ USER_MANUAL_IT.md      # Italian user guide
â””â”€â”€ messages/
    â”œâ”€â”€ en.json                # English translations
    â””â”€â”€ it.json                # Italian translations
```

## ğŸ¨ Key Features

### 1. Document Ingestion
- **Upload**: Drag & drop PDFs
- **Processing**: Automatic text extraction
- **Chunking**: Semantic splitting with overlap
- **Embedding**: OpenAI text-embedding-3-small
- **Storage**: pgvector for fast retrieval
- **Status tracking**: pending â†’ processing â†’ ingested â†’ failed

### 2. RAG Chat
- **Natural language queries**: "What is our vacation policy?"
- **Context-aware answers**: Cites specific document chunks
- **Conversation history**: Maintains context within session
- **Source attribution**: Shows which docs informed the answer
- **Streaming responses**: Real-time token-by-token delivery

### 3. SOP Generation
- **AI-powered**: Generates structured procedures from docs
- **Customizable**: Specify title, scope, and context
- **Structured output**: Purpose, prerequisites, steps, warnings, checklist
- **Export**: Markdown and PDF (with formatting)
- **Version control**: Edit, regenerate, or delete SOPs

### 4. Team Management (COMPANY_ADMIN)
- **Invite system**: Email invitations with role assignment
- **Role management**: Change roles, suspend, enable users
- **Invitation tracking**: Monitor pending, accepted, expired, revoked
- **Bulk operations**: Delete all invitations at once

### 5. Responsive Navigation
- **Desktop (â‰¥1024px)**: TopNav with all menu items
- **Mobile (<1024px)**: CommandPalette (Ctrl+K / Cmd+K)
- **Dynamic menus**: Filtered by user role
- **Keyboard shortcuts**: Fast navigation for power users

## ğŸ” API Endpoints

### Chat Query (RAG)
```typescript
POST /api/chat/query
Body: {
  query: string,           // User question
  conversationId?: uuid    // Optional, for continuing conversation
}
Response: NDJSON stream
  { type: "meta", conversationId, chunks: [...] }
  { type: "token", data: "word" }
  ...
  { type: "done" }
```

### Stripe Webhook
```typescript
POST /api/stripe/webhook
Headers: stripe-signature
Body: Stripe event payload
Handles: subscription lifecycle, checkout completion
```

### Cron Reminders
```typescript
GET /api/cron/reminders
Headers: x-cron-secret
Purpose: Send trial expiration reminders
```

## ğŸ§ª Testing & Quality

- **TypeScript**: Strict mode, no implicit `any`
- **Zod**: Runtime validation for API inputs
- **Environment validation**: Fails fast if required vars missing
- **Rate limiting**: Prevents abuse and controls costs
- **Error logging**: Sentry integration (optional)
- **CSP Headers**: Content Security Policy for XSS protection

## ğŸ“Š Performance Benchmarks

Typical latencies (production, with caching):

| Operation | Latency | Notes |
|-----------|---------|-------|
| Embed query | 50-100ms | OpenAI API call |
| Vector search | 100-200ms | PostgreSQL pgvector |
| LLM first token | 500-800ms | GPT-4.1-mini streaming |
| Full answer | 2-4s | Depends on length |
| Upload PDF | 1-3s | Depends on size |
| Chunk + embed | 5-10s | Per document |

## ğŸ“ˆ Scaling Considerations

- **Vector database**: pgvector scales to ~1M vectors per org
- **Embeddings**: Batch processing amortizes API costs
- **Rate limits**: Per-user limits prevent single-user abuse
- **Supabase**: Auto-scaling for database and storage
- **Next.js**: Edge-ready, deploy to Vercel/Cloudflare
- **OpenAI**: Tier-based rate limits, consider fine-tuned models

## ğŸ—ºï¸ Roadmap

- [ ] Advanced chunking strategies (semantic, sliding window)
- [ ] Hybrid search (vector + keyword)
- [ ] Re-ranking with cross-encoder models
- [ ] Fine-tuned embeddings for domain-specific content
- [ ] Conversation memory with summarization
- [ ] Multi-modal support (images, tables from PDFs)
- [ ] Audit logs for compliance
- [ ] Export data for GDPR compliance
- [ ] Custom RAG parameters per organization
- [ ] Integration with external tools (Slack, Teams)

## ğŸ“ Scripts

```bash
npm run dev          # Start dev server (localhost:3000)
npm run build        # Production build
npm run start        # Start production server
npm run lint         # ESLint + TypeScript check
npm run type-check   # TypeScript only
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/my-feature`
3. Follow TypeScript strict mode
4. Add tests for new features
5. Submit PR with clear description

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ”— Links

- **User Manual (EN)**: [docs/USER_MANUAL.md](docs/USER_MANUAL.md)
- **User Manual (IT)**: [docs/USER_MANUAL_IT.md](docs/USER_MANUAL_IT.md)
- **Supabase Docs**: https://supabase.com/docs
- **LangChain Docs**: https://js.langchain.com/docs
- **OpenAI API**: https://platform.openai.com/docs
- **Next.js 15**: https://nextjs.org/docs

---

**Built with** â¤ï¸ **by** [Your Name/Team]  
**Version**: 2.0  
**Last Updated**: January 2026
